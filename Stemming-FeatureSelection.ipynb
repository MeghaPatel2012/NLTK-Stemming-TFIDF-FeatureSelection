{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hDk9hDvOH7",
        "colab_type": "code",
        "outputId": "ac391577-df43-4061-a8da-158f6930bf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stemporter import PorterStemmer\n",
        "\n",
        "# Import libraries for feature \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#Change current working directory to gdrive\n",
        "%cd /gdrive\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwGQK7KAd7T",
        "colab_type": "code",
        "outputId": "b8900fea-34e9-4050-a77a-6cd288d99852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Read files\n",
        "textfile = r'/gdrive/My Drive/CIS508A5/Comments.csv'\n",
        "textData = pd.read_csv(textfile) #creates a dataframe\n",
        "\n",
        "CustInfofile = r'/gdrive/My Drive/CIS508A5/Customers.csv'\n",
        "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
        "\n",
        "print(textData.shape)\n",
        "print(CustInfoData.shape)\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 2)\n",
            "(2070, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOTk6C1Ao45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a69672dd-7378-4e4a-8982-37afa3b0f4f5"
      },
      "source": [
        "#Extract target column from Customer Info file\n",
        "y_train = CustInfoData[\"TARGET\"]\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "                     \n",
        "print(X_train.shape)\n",
        "print(textData.shape)\n",
        "textData.head()\n",
        "print(y_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 16)\n",
            "(2070, 2)\n",
            "0       Cancelled\n",
            "1         Current\n",
            "2         Current\n",
            "3         Current\n",
            "4       Cancelled\n",
            "          ...    \n",
            "2065    Cancelled\n",
            "2066    Cancelled\n",
            "2067    Cancelled\n",
            "2068    Cancelled\n",
            "2069    Cancelled\n",
            "Name: TARGET, Length: 2070, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2xV4CpMqCvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)  #Tokenization means breaking down words eg. audit becomes adit \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWYNz2Ep17l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snowball = SnowballStemmer(\"english\")\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "snowballTextData=pd.DataFrame()\n",
        "snowballTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "snowballTextData['CommentsTokenizedSnowball'] = textData['CommentsTokenized'].apply(lambda x: [snowball.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/newTextDataTS.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "snowballTextData['CommentsTokenizedSnowball'] = snowballTextData['CommentsTokenizedSnowball'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = snowballTextData.to_csv(r'/gdrive/My Drive/CIS508A5/snowballTextData-Joined.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOsUpRcKDRho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter = PorterStemmer()\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "porterTextData=pd.DataFrame()\n",
        "porterTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "porterTextData['CommentsTokenizedPorter'] = textData['CommentsTokenized'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/newTextDataTS.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "porterTextData['CommentsTokenizedPorter'] = porterTextData['CommentsTokenizedPorter'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = porterTextData.to_csv(r'/gdrive/My Drive/CIS508A5/porterTextData-Joined.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddb_WMb7D6WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lanc = LancasterStemmer()\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "lancTextData=pd.DataFrame()\n",
        "lancTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "lancTextData['CommentsTokenizedlanc'] = textData['CommentsTokenized'].apply(lambda x: [lanc.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/newTextDataTS.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "lancTextData['CommentsTokenizedlanc'] = lancTextData['CommentsTokenizedlanc'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = lancTextData.to_csv(r'/gdrive/My Drive/CIS508A5/lancTextData-Joined.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBguQloljam",
        "colab_type": "code",
        "outputId": "d43bbdae-53a1-45f1-aac2-32b553c72f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "\n",
        "TD_counts_porter = count_vect.fit_transform(porterTextData.CommentsTokenizedPorter)\n",
        "TD_counts_lanc = count_vect.fit_transform(lancTextData.CommentsTokenizedlanc)\n",
        "TD_counts_snowball = count_vect.fit_transform(snowballTextData.CommentsTokenizedSnowball)\n",
        "\n",
        "\n",
        "# Count number of stemmed words from each of the Stemmers\n",
        "print('Number of stemmed words by Porter:',TD_counts_porter.shape[1])\n",
        "print('Number of stemmed words by Lancaster:',TD_counts_lanc.shape[1])\n",
        "print('Number of stemmed words by Snowball:',TD_counts_snowball.shape[1])\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of stemmed words by Porter: 366\n",
            "Number of stemmed words by Lancaster: 364\n",
            "Number of stemmed words by Snowball: 354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI42WegFwxW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c4e470d-0155-4937-80c4-a50de7a8ec35"
      },
      "source": [
        "TD_counts_porter.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(TD_counts_porter)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts_porter.toarray())\n",
        "print(DF_TD_Counts)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "  (0, 106)\t1\n",
            "  (0, 181)\t1\n",
            "  (0, 349)\t1\n",
            "  (0, 231)\t2\n",
            "  (0, 359)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 102)\t1\n",
            "  (0, 68)\t1\n",
            "  (0, 152)\t1\n",
            "  (1, 347)\t2\n",
            "  (1, 175)\t1\n",
            "  (1, 209)\t1\n",
            "  (1, 293)\t1\n",
            "  (1, 186)\t1\n",
            "  (1, 47)\t1\n",
            "  (1, 18)\t1\n",
            "  (1, 12)\t1\n",
            "  (2, 347)\t1\n",
            "  (2, 175)\t1\n",
            "  (2, 312)\t1\n",
            "  (2, 199)\t1\n",
            "  (2, 254)\t1\n",
            "  (2, 352)\t1\n",
            "  (3, 254)\t1\n",
            "  (3, 31)\t1\n",
            "  :\t:\n",
            "  (2066, 149)\t1\n",
            "  (2066, 77)\t1\n",
            "  (2067, 231)\t1\n",
            "  (2067, 347)\t1\n",
            "  (2067, 254)\t1\n",
            "  (2067, 351)\t1\n",
            "  (2067, 282)\t1\n",
            "  (2067, 189)\t1\n",
            "  (2067, 104)\t1\n",
            "  (2067, 26)\t1\n",
            "  (2067, 196)\t1\n",
            "  (2068, 347)\t1\n",
            "  (2068, 57)\t1\n",
            "  (2068, 17)\t1\n",
            "  (2069, 231)\t1\n",
            "  (2069, 152)\t1\n",
            "  (2069, 287)\t1\n",
            "  (2069, 4)\t2\n",
            "  (2069, 189)\t1\n",
            "  (2069, 273)\t1\n",
            "  (2069, 48)\t1\n",
            "  (2069, 322)\t1\n",
            "  (2069, 305)\t1\n",
            "  (2069, 149)\t1\n",
            "  (2069, 77)\t1\n",
            "      0    1    2    3    4    5    6    ...  359  360  361  362  363  364  365\n",
            "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    2    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    2    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 366 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW5HMf-7xIOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df245d3c-ca1b-4683-87d6-4bae8bae9b1a"
      },
      "source": [
        "TD_counts_lanc.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(TD_counts_lanc)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts_lanc.toarray())\n",
        "print(DF_TD_Counts)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "  (0, 96)\t1\n",
            "  (0, 173)\t1\n",
            "  (0, 341)\t1\n",
            "  (0, 224)\t2\n",
            "  (0, 356)\t1\n",
            "  (0, 92)\t1\n",
            "  (0, 58)\t1\n",
            "  (1, 339)\t2\n",
            "  (1, 167)\t1\n",
            "  (1, 201)\t1\n",
            "  (1, 283)\t1\n",
            "  (1, 176)\t1\n",
            "  (1, 40)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 3)\t1\n",
            "  (2, 339)\t1\n",
            "  (2, 167)\t1\n",
            "  (2, 303)\t1\n",
            "  (2, 189)\t1\n",
            "  (2, 243)\t1\n",
            "  (2, 344)\t1\n",
            "  (3, 243)\t1\n",
            "  (3, 23)\t1\n",
            "  (3, 95)\t1\n",
            "  (3, 41)\t1\n",
            "  :\t:\n",
            "  (2066, 352)\t1\n",
            "  (2066, 68)\t1\n",
            "  (2067, 224)\t1\n",
            "  (2067, 339)\t1\n",
            "  (2067, 243)\t1\n",
            "  (2067, 343)\t1\n",
            "  (2067, 270)\t1\n",
            "  (2067, 180)\t1\n",
            "  (2067, 94)\t1\n",
            "  (2067, 18)\t1\n",
            "  (2067, 186)\t1\n",
            "  (2068, 339)\t1\n",
            "  (2068, 49)\t1\n",
            "  (2068, 8)\t1\n",
            "  (2069, 224)\t1\n",
            "  (2069, 41)\t1\n",
            "  (2069, 276)\t1\n",
            "  (2069, 180)\t1\n",
            "  (2069, 263)\t1\n",
            "  (2069, 42)\t1\n",
            "  (2069, 316)\t1\n",
            "  (2069, 295)\t1\n",
            "  (2069, 142)\t1\n",
            "  (2069, 352)\t1\n",
            "  (2069, 68)\t1\n",
            "      0    1    2    3    4    5    6    ...  357  358  359  360  361  362  363\n",
            "0       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "1       0    0    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 364 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpnRGXzxQFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cab4f17-03f0-4a1f-b758-9b1deb3d0147"
      },
      "source": [
        "TD_counts_snowball.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(TD_counts_snowball)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts_snowball.toarray())\n",
        "print(DF_TD_Counts)\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "  (0, 98)\t1\n",
            "  (0, 170)\t1\n",
            "  (0, 337)\t1\n",
            "  (0, 220)\t2\n",
            "  (0, 347)\t1\n",
            "  (0, 94)\t1\n",
            "  (0, 60)\t1\n",
            "  (1, 335)\t2\n",
            "  (1, 164)\t1\n",
            "  (1, 198)\t1\n",
            "  (1, 282)\t1\n",
            "  (1, 174)\t1\n",
            "  (1, 39)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 4)\t1\n",
            "  (2, 335)\t1\n",
            "  (2, 164)\t1\n",
            "  (2, 301)\t1\n",
            "  (2, 188)\t1\n",
            "  (2, 243)\t1\n",
            "  (2, 340)\t1\n",
            "  (3, 243)\t1\n",
            "  (3, 23)\t1\n",
            "  (3, 97)\t1\n",
            "  (3, 334)\t1\n",
            "  :\t:\n",
            "  (2066, 310)\t1\n",
            "  (2066, 294)\t1\n",
            "  (2066, 139)\t1\n",
            "  (2066, 69)\t1\n",
            "  (2067, 220)\t1\n",
            "  (2067, 335)\t1\n",
            "  (2067, 243)\t1\n",
            "  (2067, 339)\t1\n",
            "  (2067, 271)\t1\n",
            "  (2067, 178)\t1\n",
            "  (2067, 96)\t1\n",
            "  (2067, 18)\t1\n",
            "  (2067, 185)\t1\n",
            "  (2068, 335)\t1\n",
            "  (2068, 49)\t1\n",
            "  (2068, 9)\t1\n",
            "  (2069, 220)\t1\n",
            "  (2069, 276)\t1\n",
            "  (2069, 178)\t1\n",
            "  (2069, 262)\t1\n",
            "  (2069, 40)\t1\n",
            "  (2069, 310)\t1\n",
            "  (2069, 294)\t1\n",
            "  (2069, 139)\t1\n",
            "  (2069, 69)\t1\n",
            "      0    1    2    3    4    5    6    ...  347  348  349  350  351  352  353\n",
            "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1       0    0    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8TZYnAxQbP",
        "colab_type": "code",
        "outputId": "54da5327-b5f6-43ee-c5c3-d5414961d012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts_porter)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF_porter=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF_porter)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 366)\n",
            "      0    1    2    3         4    5    ...  360  361  362  363  364  365\n",
            "0     0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...       ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.407251  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.407251  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 366 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk8U8dW2xdbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "21987abe-8035-49e0-d2e0-2019124e69d9"
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts_lanc)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF_lanc=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF_lanc)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 364)\n",
            "      0    1    2         3    4    5    6    ...  357  358  359  360  361  362  363\n",
            "0     0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.275669  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 364 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGw0wivuxlCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts_snowball)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF_snowball=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF_snowball)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2owIUD6_eYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature selection Filter type\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=150).fit_transform(DF_TF_IDF_porter,y_train)\n",
        "new_DF_TF_IDF.shape\n",
        "\n",
        "DF_TF_IDF_Filter= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_Filter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geVCLka8xxjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Construct a Random Forest Classifier on the filter text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_Filter,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_Filter, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_Filter)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWvetxEQz0G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa2c5321-db6e-48f3-d141-1ef6a0050423"
      },
      "source": [
        "#Feature selection Wrapper Type\n",
        "\n",
        "# Build RF classifier to use in feature selection\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=15,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "sfs1 = sfs1.fit(DF_TF_IDF, y_train)\n",
        "feat_cols = list(sfs1.k_feature_idx_)\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 354 out of 354 | elapsed:   26.9s finished\n",
            "\n",
            "[2019-12-13 17:03:57] Features: 1/15 -- score: 0.6159469643320449[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 353 out of 353 | elapsed:   27.1s finished\n",
            "\n",
            "[2019-12-13 17:04:24] Features: 2/15 -- score: 0.6202889643988454[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 352 out of 352 | elapsed:   27.7s finished\n",
            "\n",
            "[2019-12-13 17:04:52] Features: 3/15 -- score: 0.6241525402589408[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 351 out of 351 | elapsed:   28.3s finished\n",
            "\n",
            "[2019-12-13 17:05:20] Features: 4/15 -- score: 0.6265703329865062[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 350 out of 350 | elapsed:   28.8s finished\n",
            "\n",
            "[2019-12-13 17:05:49] Features: 5/15 -- score: 0.6289846278471081[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 349 out of 349 | elapsed:   29.1s finished\n",
            "\n",
            "[2019-12-13 17:06:18] Features: 6/15 -- score: 0.6294700534249149[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 348 out of 348 | elapsed:   29.5s finished\n",
            "\n",
            "[2019-12-13 17:06:48] Features: 7/15 -- score: 0.6294700534249149[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 347 out of 347 | elapsed:   29.5s finished\n",
            "\n",
            "[2019-12-13 17:07:18] Features: 8/15 -- score: 0.629957812793089[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 346 out of 346 | elapsed:   30.1s finished\n",
            "\n",
            "[2019-12-13 17:07:48] Features: 9/15 -- score: 0.6309216682147758[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 345 out of 345 | elapsed:   30.0s finished\n",
            "\n",
            "[2019-12-13 17:08:18] Features: 10/15 -- score: 0.6309216682147757[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 344 out of 344 | elapsed:   30.4s finished\n",
            "\n",
            "[2019-12-13 17:08:48] Features: 11/15 -- score: 0.6309216682147758[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 343 out of 343 | elapsed:   30.6s finished\n",
            "\n",
            "[2019-12-13 17:09:19] Features: 12/15 -- score: 0.6309216682147758[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 342 out of 342 | elapsed:   30.5s finished\n",
            "\n",
            "[2019-12-13 17:09:49] Features: 13/15 -- score: 0.6314059297159865[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 341 out of 341 | elapsed:   30.9s finished\n",
            "\n",
            "[2019-12-13 17:10:20] Features: 14/15 -- score: 0.6314059297159865[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 340 out of 340 | elapsed:   30.8s finished\n",
            "\n",
            "[2019-12-13 17:10:51] Features: 15/15 -- score: 0.6323721132908656"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJDjcalCz6c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DF_TF_IDF_Wrap=DF_TF_IDF.iloc[:,feat_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ0X6ski1gR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0087e8cc-9e27-4436-ff3e-fd10214e1502"
      },
      "source": [
        "#Construct a Random Forest Classifier on the wrap text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_Wrap,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_Wrap, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_Wrap)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.632367\n",
            "Confusion Matrix:\n",
            "[[  65  739]\n",
            " [  22 1244]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.75      0.08      0.15       804\n",
            "     Current       0.63      0.98      0.77      1266\n",
            "\n",
            "    accuracy                           0.63      2070\n",
            "   macro avg       0.69      0.53      0.46      2070\n",
            "weighted avg       0.67      0.63      0.53      2070\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq_7eeQOz7xk",
        "colab_type": "code",
        "outputId": "253e6121-6691-46d5-e8e8-ab19a6e68b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#Merge files\n",
        "\n",
        "print(CustInfoData.shape)\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "print(X_train.shape)\n",
        "combined=pd.concat([X_train, DF_TF_IDF_SelectedFeatures], axis=1)\n",
        "print(combined.shape)\n",
        "print(combined)\n",
        "export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508A5/Combined-Cust+TFIDF+SelectedFeatures.csv')\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 17)\n",
            "(2070, 16)\n",
            "(2070, 26)\n",
            "        ID Sex Status  Children  Est_Income  ...    5    6    7    8    9\n",
            "0        1   F      S         1    38000.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "1        6   M      M         2    29616.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2        8   M      M         0    19732.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "3       11   M      S         2       96.33  ...  0.0  0.0  0.0  0.0  0.0\n",
            "4       14   F      M         2    52004.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "...    ...  ..    ...       ...         ...  ...  ...  ...  ...  ...  ...\n",
            "2065  3821   F      S         0    78851.30  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2066  3822   F      S         1    17540.70  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2067  3823   F      M         0    83891.90  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2068  3824   F      M         2    28220.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2069  3825   F      S         0    28589.10  ...  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmJYiWrPkRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do one Hot encoding for categorical features\n",
        "X_cat = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
        "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
        "print(X_cat)\n",
        "combined_one_hot = pd.get_dummies(combined,columns=X_cat)\n",
        "print(combined_one_hot.shape)\n",
        "export_csv= combined_one_hot.to_csv(r'/gdrive/My Drive/CIS508A5/combined_one_hot.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKd66edUzBkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(combined_one_hot, y_train, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMpFKQAVuV1",
        "colab_type": "code",
        "outputId": "aba10e07-374b-4548-8eea-277946cd4485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#Construct a Random Forest Classifier on combined data\n",
        "#clf1=RandomForestClassifier()\n",
        "RF_Comb = clf.fit(X_Train,Y_Train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Train,Y_Train)))\n",
        "rf_predictions = clf.predict(X_Train)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Train, rf_predictions))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.986111\n",
            "Confusion Matrix:\n",
            "[[640   7]\n",
            " [ 16 993]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.98      0.99      0.98       647\n",
            "     Current       0.99      0.98      0.99      1009\n",
            "\n",
            "    accuracy                           0.99      1656\n",
            "   macro avg       0.98      0.99      0.99      1656\n",
            "weighted avg       0.99      0.99      0.99      1656\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgP-QlP17rNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "19369f4f-8a81-4503-d1ff-9ac3a2cadfdf"
      },
      "source": [
        "#CONTRUCT A GRADIENT BOOSTING MODEL==============================\n",
        "abc =GradientBoostingClassifier()\n",
        "abc.fit(X_Train,Y_Train)\n",
        "abc_predict=abc.predict(X_Test)\n",
        "print('Final prediction score for ensemble methods: [%.8f]' % acc(Y_Test, abc_predict))\n",
        "print(\"Confusion Matrix after STACKING for Boosting:\")\n",
        "print(confusion_matrix(Y_Test, abc_predict))\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(Y_Test, abc_predict))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final prediction score for ensemble methods: [0.85748792]\n",
            "Confusion Matrix after STACKING for Boosting:\n",
            "[[120  37]\n",
            " [ 22 235]]\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.85      0.76      0.80       157\n",
            "     Current       0.86      0.91      0.89       257\n",
            "\n",
            "    accuracy                           0.86       414\n",
            "   macro avg       0.85      0.84      0.85       414\n",
            "weighted avg       0.86      0.86      0.86       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfcMblEg7zra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f6a669a3-b571-4438-aceb-f3ef60640776"
      },
      "source": [
        "#Gradient Boosting Hyperparam Tuning ============================================================\n",
        "search_grid={'n_estimators':[5,10,20,30,50],'learning_rate':[0.02,0.5]}\n",
        "#Randomized Search for hyperparameter tuning\n",
        "abc_random = RandomizedSearchCV(abc,search_grid,n_iter=15)\n",
        "abc_random.fit(X_Train,Y_Train)\n",
        "grid_parm_abc=abc_random.best_params_\n",
        "print(grid_parm_abc)\n",
        "\n",
        "#Construct Gradient Boosting Trees using the best parameters\n",
        "abc= GradientBoostingClassifier(**grid_parm_abc)\n",
        "abc.fit(X_Train,Y_Train)\n",
        "abc_predict = abc.predict(X_Test)\n",
        "print('Final prediction score for ensemble methods: [%.8f]' % acc(Y_Test, abc_predict))\n",
        "print(\"Confusion Matrix after STACKING for Boosting:\")\n",
        "print(confusion_matrix(Y_Test,abc_predict))\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(Y_Test,abc_predict))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': 50, 'learning_rate': 0.5}\n",
            "Final prediction score for ensemble methods: [0.86231884]\n",
            "Confusion Matrix after STACKING for Boosting:\n",
            "[[123  34]\n",
            " [ 23 234]]\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.84      0.78      0.81       157\n",
            "     Current       0.87      0.91      0.89       257\n",
            "\n",
            "    accuracy                           0.86       414\n",
            "   macro avg       0.86      0.85      0.85       414\n",
            "weighted avg       0.86      0.86      0.86       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}